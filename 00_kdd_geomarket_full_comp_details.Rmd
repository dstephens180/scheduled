---
title: "KDD rentaldata_dev: Geomarket Full & Comps Details Data Extract"
---


# LIBRARIES
```{r setup, include=FALSE}

# sql connect
library(odbc)
library(DBI)
library(arrow)

# core packages
library(tidyverse)
library(timetk)
library(tidyquant)
library(janitor)
library(lubridate)
library(stringi)
library(jsonlite)
library(tidyjson)
library(data.table)

library(blastula)



date     <- as.Date(today())
today    <- as.Date(today()+1)
today_7  <- as.Date(today()+7)
today_14 <- as.Date(today()+14)
today_30 <- as.Date(today()+30)
today_90 <- as.Date(today()+90)

options(scipen = 9999)
options(dplyr.summarise.inform = FALSE)


ds_conn <- 
  dbConnect(
    RMariaDB::MariaDB(),
    host     = "rented-datascience.ccem57tyvghb.us-west-2.rds.amazonaws.com",
    dbname   = "rentaldata_dev",
    username = "david",
    password = "NZg*2ptvQ@X$uR")


knitr::opts_chunk$set(echo = TRUE)
```




# --
# 0.0 DATA

## KDD Recohort Raw Data
```{r}
full_recohort_raw <-
  read_rds("C:\\Users\\DavidStephens\\Desktop\\Github\\artr_replica\\shiny\\01_healthscore\\full_kdd_recohort.rds") %>%
  distinct(id, .keep_all = T)
```




# --
# 0.1 IMPORT ART DATA
```{r}
geomarket_cleaned_tbl <-
  read_rds("C:\\Users\\DavidStephens\\Desktop\\Github\\scheduled\\00_data\\geomarket_cleaned_tbl.rds") %>%
  select(
    -full,
    -comps,
    -starts_with("adr"), 
    -starts_with("booked")
  )


# all art id's with account_id assignment
accounts_listings_raw <- 
  geomarket_cleaned_tbl %>% 
  select(account_id, id) %>%
  distinct(id, .keep_all = T)
```




# --
# 0.2 JOIN ART DATA WITH KDD RECOHORTED FULL/COMPS
```{r}
geomarket_recohorted_cleaned_tbl <- 
  geomarket_cleaned_tbl %>%
  left_join(full_recohort_raw, by = "id")
```




# --
# 1.0 DATA PREP

## Prepare Data for Joining after Loop
```{r}
art_listing_full_comps_tbl <- 
  geomarket_recohorted_cleaned_tbl %>%
  select(id, full, comps) %>%
  rowid_to_column(var = "rowid_column")

```


```{r}
# full data only 
full_extract_longer <- 
  art_listing_full_comps_tbl %>%
  select(-comps) %>%
  unnest(full) %>%
  rename(listing_id = full) %>%
  mutate(type = "full")



# comps data only 
comps_extract_longer <- 
  art_listing_full_comps_tbl %>%
  select(-full) %>%
  unnest(comps) %>%
  rename(listing_id = comps) %>%
  mutate(type = "comps")



# this is your full tbl that you will use to join after the for loop
full_comps_extract_longer <- full_extract_longer %>%
  bind_rows(comps_extract_longer)
```




# --
# 2.0 DATA EXTRACT

## Unique listing_id
```{r}
# get unique listing_ids
unique_listing_ids <- full_extract_longer %>%
  bind_rows(comps_extract_longer) %>%
  distinct(listing_id, .keep_all = F) %>%
  rowid_to_column(var = "identifier")


unique_listing_ids_vector <- as.vector(unique_listing_ids %>% pull(listing_id))
sample_unique_ids_vector <- unique_listing_ids_vector[1:5]
```


## Filter/Join SQL Tables & Extract Data
**Long-running script**
```{r}
calendar <- tbl(ds_conn, "combined_calendar")
active_listings <- tbl(ds_conn, "combined_active")
location_listings <- tbl(ds_conn, "combined_listings")



# sample or all unique listings?
listing_vector_used <- unique_listing_ids_vector



# extract listing details
listing_descriptive_details <- calendar %>%
  inner_join(active_listings, by = "listing_id") %>%
  inner_join(location_listings, by = "listing_id") %>%
  filter(listing_id %in% local(listing_vector_used)) %>%
  select(listing_id, bedrooms, bathrooms, sleeps, rating, reviews, room_type, latitude, longitude) %>%
  distinct(listing_id, .keep_all = T) %>%
  collect()



# summarize availability and metrics
geomarket_metrics <- calendar %>%
  inner_join(active_listings, by = "listing_id") %>%
  inner_join(location_listings, by = "listing_id") %>%
  filter(listing_id %in% local(listing_vector_used)) %>%
  select(listing_id, date, rate, available) %>%
  
  dplyr::mutate(
    
    # days out availability
    days_out_7  = ifelse(between(date, today, today_7), available, NA),
    days_out_14 = ifelse(between(date, today, today_14), available, NA),
    days_out_30 = ifelse(between(date, today, today_30), available, NA),
    days_out_90 = ifelse(between(date, today, today_90), available, NA),
    
    # adr next n days
    adr_7_days  = ifelse(between(date, today, today_7), rate, NA),
    adr_14_days = ifelse(between(date, today, today_14), rate, NA),
    adr_30_days = ifelse(between(date, today, today_30), rate, NA),
    adr_90_days = ifelse(between(date, today, today_90), rate, NA)
    
  ) %>%
  
  # summarize by listing_id
  group_by(listing_id) %>%
  summarise(
    mean_rate = round(mean(rate, na.rm = T), 0),
    std_dev   = round(sd(rate, na.rm = T), 0),
    
    # booked next n days (1 - availability)
    booked_7_days_out  = 1 - mean(days_out_7, na.rm = T),
    booked_14_days_out = 1 - mean(days_out_14, na.rm = T),
    booked_30_days_out = 1 - mean(days_out_30, na.rm = T),
    booked_90_days_out = 1 - mean(days_out_90, na.rm = T),
    
    # adr next n days
    adr_7_days  = round(mean(adr_7_days, na.rm = T)),
    adr_14_days = round(mean(adr_14_days, na.rm = T)),
    adr_30_days = round(mean(adr_30_days, na.rm = T)),
    adr_90_days = round(mean(adr_90_days, na.rm = T))
    
  ) %>%
  ungroup() %>%
  distinct(listing_id, .keep_all = T) %>%
  collect()



# join the data
geomarket_grouped_extract <- listing_descriptive_details %>%
  left_join(geomarket_metrics, by = "listing_id")
  
```



## Join with Original Table
```{r}

geomarket_full_comps_details_tbl <- full_comps_extract_longer %>%
  left_join(geomarket_grouped_extract, by = "listing_id") %>%
  relocate(listing_id) %>%
  drop_na(bedrooms)

```




# --
# 3.0 SAVE & DISCONNECT

```{r}

# connect & upload
ds_conn <- 
  DBI::dbConnect(
    RMariaDB::MariaDB(),
    host     = "rented-datascience.ccem57tyvghb.us-west-2.rds.amazonaws.com",
    dbname   = "rentaldata_dev",
    username = "david",
    password = "NZg*2ptvQ@X$uR"
  )


## handy-dandy function
dbWriteTableMySQLFast<- function(conn, df, tbl_name) {
  dbRemoveTable(conn, tbl_name, fail_if_missing = FALSE)
  dbCreateTable(conn, tbl_name, fields = df)
  
  f <- tempfile()
  write_csv(df, file = f, na = "NULL")
  
  dbWriteTable(conn, tbl_name, f, append = TRUE)

  unlink(f)
}


# upload
dbWriteTableMySQLFast(
  conn     = ds_conn, 
  df       = geomarket_full_comps_details_tbl, 
  tbl_name = "00_kdd_comps_full_geomarket_details_tbl"
)
```




## Save as parquet
```{r}
# live file in scheduled/00_data folder
write_parquet(geomarket_full_comps_details_tbl, "C:\\Users\\DavidStephens\\Desktop\\Github\\scheduled\\00_data\\kdd_comps_full_geomarket_details_tbl.parquet")


# live file for shiny in art_replica/shiny folder
write_parquet(geomarket_full_comps_details_tbl, "C:\\Users\\DavidStephens\\Desktop\\Github\\artr_replica\\shiny\\kdd_comps_full_geomarket_details_tbl.parquet")


# time-stamped archive file
write_parquet(geomarket_full_comps_details_tbl, str_glue("C:\\Users\\DavidStephens\\Desktop\\Github\\scheduled\\00_data\\archive\\{date}_kdd_comps_full_geomarket_details_tbl.parquet"))
```





## Disconnect SQL
```{r}
dbDisconnect(ds_conn)
```




# --
# 4.0 EMAIL NOTIFICATION
```{r}

date_time <- add_readable_time()

body_text <- 
  md(str_glue(
    
"
Good news!

The **KDD Geomarket Full & Comps Details Data Extract** script ran successfully on {date_time}.

"))


footer_text <- md("sent via the [blastula](https://rstudio.github.io/blastula) R package")



email <- compose_email(
  body = body_text,
  footer = footer_text
)


# send the email
email %>%
  smtp_send(
    from = "dstephens@tnsinc.com",
    to = "dstephens@tnsinc.com", 
    subject = str_glue("KDD Geomarket Full & Comps Details"),
    credentials = creds_file(file = "C:\\Users\\DavidStephens\\Desktop\\Github\\scheduled\\gmail_creds")
  )
```










