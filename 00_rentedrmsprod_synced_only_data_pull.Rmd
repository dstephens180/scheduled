---
title: "rentedrmsprod: Synced Listings Only"
---


# LIBRARIES
```{r setup, include=FALSE}

# sql connect
library(odbc)
library(DBI)
library(arrow)

# core packages
library(tidyverse)
library(dbplyr)
library(timetk)
library(tidyquant)
library(janitor)
library(lubridate)
library(stringi)
library(jsonlite)
library(tidyjson)
library(data.table)

library(blastula)



date <- today()
options(scipen = 9999)

beginning_url <- '<img src="'
ending_url    <- '" height="70"></img>'


PROD_HOST    <- Sys.getenv("PROD_HOST")
PROD_DB_NAME <- Sys.getenv("PROD_DB_NAME")
PROD_ID      <- Sys.getenv("PROD_ID")
PROD_PW      <- Sys.getenv("PROD_PW")

prod_conn <- dbConnect(RMariaDB::MariaDB(),
                        host     = PROD_HOST,
                        dbname   = PROD_DB_NAME,
                        username = PROD_ID,
                        password = PROD_PW)

knitr::opts_chunk$set(echo = TRUE)

```




# --
# 0.0 DATA IMPORT
## Create vector of all full-service clients
```{r}
full_service <- c("Arizona Elite Properties", 
                  "Bear Lake Luxury", 
                  "BookStayHop LLC", 
                  "Carolina Stays, Inc", 
                  "Clemson Vacation Rentals",
                  "Crown Point - Exclusive Island Rentals",
                  "Fran Maxon Real Estate", 
                  "Good Life Vacations", 
                  "Gulf Coast Beachfronts",
                  "Hodnett Cooper", 
                  "Marina Hawaii",
                  "New Wave Vacation Rentals", 
                  "Premium Beach Condos",
                  "ResorticaHawaii.com",
                  "Santa Barbara Vacation Rentals",
                  "Sloane Realty Vacations",
                  "South Key Management Co.",
                  "Stellar Beach Rentals", 
                  "Vacation Management Solutions LLC", 
                  "Vacation Rentals Park City", 
                  "VB Homes (CEBT Properties, LLC)"
                  )
```




# --
# 1.0 SQL EXTRACTS
## pull art_listings: geomarket & rates
```{sql connection=prod_conn, output.var="prod_art_listings"}
select cast(id as CHAR), cast(accounts_id as CHAR), cast(listing_name as CHAR), cast(geomarket as CHAR), cast(rates as CHAR), cast(availability as CHAR)
from art_listings
where sync = 1

```


## select, non-synced accounts
```{sql connection=prod_conn, output.var="prod_art_select_listings"}
select cast(id as CHAR), cast(accounts_id as CHAR), cast(listing_name as CHAR), cast(geomarket as CHAR), cast(rates as CHAR)
from art_listings
where (
accounts_id = 1320 OR
accounts_id = 1363 OR
accounts_id = 1365 OR
accounts_id = 1380 OR
accounts_id = 1386 OR
accounts_id = 1390 OR
accounts_id = 1391 OR
accounts_id = 1392 OR
accounts_id = 1405 OR
accounts_id = 1413 OR
accounts_id = 1414 OR
accounts_id = 1416)

```





## pull art_accounts
```{sql connection=prod_conn, output.var="prod_art_accounts"}
select id, account_name, deactivation_reason, deactivation_date
from art_accounts
```





## pull art_listings: details only
```{sql connection=prod_conn, output.var="prod_art_listings_details"}
select cast(id as CHAR), cast(details as CHAR)
from art_listings
where sync = 1

```


## select, non-synced accounts
```{sql connection=prod_conn, output.var="prod_art_select_listings_details"}
select cast(id as CHAR), cast(details as CHAR)
from art_listings
where (
accounts_id = 1320 OR
accounts_id = 1363 OR
accounts_id = 1365 OR
accounts_id = 1380 OR
accounts_id = 1386 OR
accounts_id = 1390 OR
accounts_id = 1391 OR
accounts_id = 1392 OR
accounts_id = 1405 OR
accounts_id = 1413 OR
accounts_id = 1414 OR
accounts_id = 1416)

```








## Details: Spread & Prepare
```{r}
prod_art_listings_unnested <- prod_art_listings_details %>%
  bind_rows(prod_art_select_listings_details) %>%
  rename(
    id      = `cast(id as CHAR)`,
    details = `cast(details as CHAR)`
  ) %>% 
  
  # extract json details
  rowwise() %>%
  mutate(details_extract = list(fromJSON(details))) %>%
  ungroup() %>%
  
  # unnest wider
  unnest_wider(details_extract, simplify = T) %>%
  select(-details, -pricelabs, -listing_name, -airbnb_listing_id, -airbnb_user_id) %>%
  
  # unlist all columns that are still lists
  apply(2, function(y) sapply(y, function(x) paste(unlist(x), collapse = ", "))) %>%
  as_tibble() %>%
  
  # replace NA text with NA's
  mutate_all(~na_if(., "")) %>%
  mutate(across(where(is.character), na_if, "NA")) %>%
  
  # convert clean up columns
  mutate(
    id = as.integer(id),
    full_baths = as.numeric(full_baths) + as.numeric(half_baths)
  ) %>%
  select(-half_baths, -baths) %>%
  mutate_at(c("beds", "guests", "rating", "reviews", "gap_stay", "latitude", "longitude", "min_stay", "bed_count", "min_stay_weekends", "min_stay_close", "min_stay_close_days"), as.numeric)


prod_art_listings_unnested %>% glimpse()

```


## Unnest rates
```{r}

prod_art_rates_unnested <- prod_art_listings %>%
  bind_rows(prod_art_select_listings) %>%
  rename(
    id           = `cast(id as CHAR)`,
    account_id   = `cast(accounts_id as CHAR)`,
    listing_name = `cast(listing_name as CHAR)`,
    geomarket    = `cast(geomarket as CHAR)`,
    rates        = `cast(rates as CHAR)`,
    availability = `cast(availability as CHAR)`
  ) %>%
  
  # remove any "null" in geomarket column
  select(id, geomarket, rates) %>%
  filter(geomarket != "null") %>%
  
  # select only id & rates
  select(id, rates) %>%
  rowid_to_column(var = "rowid_column")




prod_art_rates_vector <- as.vector(prod_art_rates_unnested %>% pull(rowid_column))



### FOR LOOP ###
# create empty vector
key_metrics_data = c()


for (i in prod_art_rates_vector) {
  
  tryCatch({
  
  selected_column <- i
  
  inputs <- 
    as.vector(
      prod_art_rates_unnested %>%
        dplyr::filter(rowid_column == selected_column) %>%
        dplyr::pull(rates) %>%
        jsonlite::fromJSON()) %>%
    
    # spread selected values as jstring to avoid "" throwing errors
    tidyjson::spread_values(
      max_rate      = tidyjson::jstring(max_rate),
      min_rate      = tidyjson::jstring(min_rate),
      base_rate     = tidyjson::jstring(base_rate),
      suggested_base_rate = tidyjson::jstring(suggested_base_rate),
      min_base_rate = tidyjson::jstring(min_base_rate),
      max_base_rate = tidyjson::jstring(max_base_rate)
    ) %>%
    tibble::as_tibble() %>%
    
    # filter is for [1], or $inputs
    dplyr::filter(document.id == "1")
  
  
  
  ### OUTPUTS ###
  outputs <- 
    as.vector(
      prod_art_rates_unnested %>%
        dplyr::filter(rowid_column == selected_column) %>%
        dplyr::pull(rates) %>%
        jsonlite::fromJSON())[[2]] %>%
    
    # spread selected values
    tidyjson::spread_values(
      model_rate     = tidyjson::jnumber(model_rate),
      predicted_rate = tidyjson::jnumber(predicted_rate)
    ) %>%
    
    # mean model & adjusted rates
    dplyr::summarise(
      mean_model_rate    = round(mean(model_rate)),
      mean_adjusted_rate = round(mean(predicted_rate)),
      adr_7_days  = mean(head(predicted_rate, 7), na.rm = T),
      adr_14_days = mean(head(predicted_rate, 14), na.rm = T),
      adr_30_days = mean(head(predicted_rate, 30), na.rm = T),
      adr_90_days = mean(head(predicted_rate, 90), na.rm = T)
    )
  
  
  combined_tbl <- bind_cols(inputs, outputs) %>%
    
    # add rowid column for later joining
    mutate(rowid_column = i) %>%
    select(-document.id) %>%
    
    # clean up jstring (avoiding errors) to numeric
    mutate(
      max_rate = ifelse(max_rate == "", NA, as.numeric(max_rate)),
      min_rate = ifelse(min_rate == "", NA, as.numeric(min_rate)),
      base_rate = ifelse(base_rate == "", NA, as.numeric(base_rate)),
      suggested_base_rate = ifelse(suggested_base_rate == "", NA, as.numeric(suggested_base_rate)),
      min_base_rate = ifelse(min_base_rate == "", NA, as.numeric(min_base_rate)),
      max_base_rate = ifelse(max_base_rate == "", NA, as.numeric(max_base_rate)))
  
  
  # bind rows of all_container_data
  key_metrics_data <- bind_rows(key_metrics_data, combined_tbl)
  
  }, error=function(e){})
  
}



# join with id's
key_metrics_cleaned <- key_metrics_data %>%
  left_join(prod_art_rates_unnested %>% select(rowid_column, id), by = "rowid_column") %>%
  select(-rowid_column) %>%
  relocate(id)
```



## Unnest availability
```{r}

prod_art_availability_unnested <- prod_art_listings %>%
  bind_rows(prod_art_select_listings) %>%
  rename(
    id           = `cast(id as CHAR)`,
    account_id   = `cast(accounts_id as CHAR)`,
    listing_name = `cast(listing_name as CHAR)`,
    geomarket    = `cast(geomarket as CHAR)`,
    rates        = `cast(rates as CHAR)`,
    availability = `cast(availability as CHAR)`
  ) %>%
  
  # remove any "null" in geomarket column
  select(id, geomarket, availability) %>%
  filter(geomarket != "null") %>%
  
  # select only id & availability
  select(id, availability) %>%
  rowid_to_column(var = "rowid_column")




prod_art_availability_vector <- as.vector(prod_art_availability_unnested %>% pull(rowid_column))



### FOR LOOP ###
# create empty vector
availability_data = c()


for (i in prod_art_availability_vector) {
  
  tryCatch({
  
  selected_column <- i
  
  combined_tbl <- 
    as.vector(
      prod_art_availability_unnested %>%
        dplyr::filter(rowid_column == selected_column) %>%
        dplyr::pull(availability) %>%
        jsonlite::fromJSON()) %>%
    
    # spread selected values
    tidyjson::spread_values(
      available     = tidyjson::jnumber(available)
    ) %>%
    
    # calculate next n days booked (1 - availability)
    dplyr::summarise(
      booked_7_days_out  = 1 - mean(head(available, 7), na.rm = T),
      booked_14_days_out = 1 - mean(head(available, 14), na.rm = T),
      booked_30_days_out = 1 - mean(head(available, 30), na.rm = T),
      booked_90_days_out = 1 - mean(head(available, 90), na.rm = T)
    ) %>%
    
    # add rowid column for later joining
    mutate(rowid_column = i) %>%
  
    # clean up jstring (avoiding errors) to numeric
    mutate(
      booked_7_days_out = ifelse(booked_7_days_out == "", NA, as.numeric(booked_7_days_out)),
      booked_14_days_out = ifelse(booked_14_days_out == "", NA, as.numeric(booked_14_days_out)),
      booked_30_days_out = ifelse(booked_30_days_out == "", NA, as.numeric(booked_30_days_out)),
      booked_90_days_out = ifelse(booked_90_days_out == "", NA, as.numeric(booked_90_days_out))
    )
    
  
  
  # bind rows of all_container_data
  availability_data <- bind_rows(availability_data, combined_tbl)
  
  }, error=function(e){})
  
}



# join with id's
availability_cleaned <- availability_data %>%
  left_join(prod_art_rates_unnested %>% select(rowid_column, id), by = "rowid_column") %>%
  select(-rowid_column) %>%
  relocate(id)
```


## Join listings & accounts
```{r}
prod_art_listings_cleaned <- prod_art_listings %>%
  bind_rows(prod_art_select_listings) %>%
  rename(
    id           = `cast(id as CHAR)`,
    account_id   = `cast(accounts_id as CHAR)`,
    listing_name = `cast(listing_name as CHAR)`,
    geomarket    = `cast(geomarket as CHAR)`,
    rates        = `cast(rates as CHAR)`,
    availability = `cast(availability as CHAR)`
  )


# both deactivation_reason/date should be NA/NULL values to be truly activated
prod_art_accounts_cleaned <- prod_art_accounts %>%
  mutate(full_service = ifelse(account_name %in% full_service, "full-service", "art-only")) %>%
  filter(is.na(deactivation_date) & is.na(deactivation_reason)) %>%
  mutate(
    account_id = as.character(id),
    active     = "active"
  ) %>%
  select(account_id, account_name, active, full_service)



prod_listings_joined <- prod_art_listings_cleaned %>%
  left_join(prod_art_accounts_cleaned, by = "account_id") %>%
  left_join(key_metrics_cleaned, by = "id") %>%
  left_join(availability_cleaned, by = "id") %>%
  filter(active == "active")



prod_listings_joined %>% glimpse()
```




# --
# 2.0 DATA PREP
## Geomarket Cleaning
```{r}
# small sample for testing
sample_tbl <- prod_listings_joined %>% head(10) %>% rowid_to_column()



# unnest & apply (over columns) and sapply (iterate over lists)
geomarket_cleaned_tbl <- prod_listings_joined %>%
  rowwise() %>%
  mutate(geomarket_extract = list(fromJSON(geomarket))) %>%
  ungroup() %>%
  unnest_wider(geomarket_extract) %>%
  apply(2, function(y) sapply(y, function(x) paste(unlist(x), collapse = ", "))) %>%
  as_tibble() %>%
  select(
    id, 
    account_id, 
    account_name, 
    listing_name, 
    full_service, 
    full, 
    comps,
    max_rate, min_rate, 
    base_rate, 
    suggested_base_rate,
    min_base_rate,
    max_base_rate, 
    mean_model_rate, 
    mean_adjusted_rate,
    adr_7_days,
    adr_14_days,
    adr_30_days,
    adr_90_days,
    booked_7_days_out,
    booked_14_days_out,
    booked_30_days_out,
    booked_90_days_out
  ) %>%
  mutate(
    id = as.integer(id),
    max_rate = as.numeric(max_rate),
    min_rate = as.numeric(min_rate),
    base_rate = as.numeric(base_rate),
    suggested_base_rate = as.numeric(suggested_base_rate),
    min_base_rate = as.numeric(min_base_rate),
    max_base_rate = as.numeric(max_base_rate),
    mean_model_rate = as.numeric(mean_model_rate),
    mean_adjusted_rate = as.numeric(mean_adjusted_rate),
    adr_7_days  = as.numeric(adr_7_days),
    adr_14_days = as.numeric(adr_14_days),
    adr_30_days = as.numeric(adr_30_days),
    adr_90_days = as.numeric(adr_90_days),
    booked_7_days_out  = as.numeric(booked_7_days_out),
    booked_14_days_out = as.numeric(booked_14_days_out),
    booked_30_days_out = as.numeric(booked_30_days_out),
    booked_90_days_out = as.numeric(booked_90_days_out)
  ) %>%
  distinct(id, .keep_all = T) %>%
  filter(full  != "",
         comps != "")



# this is used for full/comp sets
write_rds(geomarket_cleaned_tbl, "00_data/geomarket_cleaned_tbl.rds")
write_rds(geomarket_cleaned_tbl, str_glue("00_data/archive/{date}_geomarket_cleaned_tbl.rds"))

geomarket_cleaned_tbl <- read_rds("00_data/geomarket_cleaned_tbl.rds")
```



## Join and finalize
```{r}
data_full_cleaned_tbl <- geomarket_cleaned_tbl %>%
  left_join(prod_art_listings_unnested, by = "id") %>%
  filter(account_name != "Art Demo - Smoky Mountains") %>%
  distinct(id, .keep_all = T) %>%
  
  # add thumbnail
  mutate(thumbnail = str_glue("{beginning_url}{img_url}{ending_url}")) %>%
  relocate(thumbnail) %>%
  
  # change 0 to NA in lat/long
  mutate_at(c('latitude', 'longitude'), ~na_if(., 0)) %>%
  
  # clean city names
  mutate(city = make_clean_names(city, allow_dupes = T)) %>%
  mutate(city = sub("^(.)", "\\U\\1", gsub("_", " ", city), perl = TRUE)) %>%
  mutate(city = str_to_title(city)) %>%
  mutate(city = ifelse(str_detect("Na", city), NA, city)) %>%
  
  # select only what matters
  select(
    thumbnail,
    id, 
    account_id, 
    account_name, 
    listing_name, 
    full_service, 
    max_rate, 
    min_rate, 
    base_rate, 
    suggested_base_rate,
    min_base_rate, 
    max_base_rate, 
    mean_model_rate, 
    mean_adjusted_rate,
    adr_7_days,
    adr_14_days,
    adr_30_days,
    adr_90_days,
    booked_7_days_out,
    booked_14_days_out,
    booked_30_days_out,
    booked_90_days_out,
    city,
    state,
    zip,
    country,
    room_type,
    beds,
    full_baths,
    guests,
    rating,
    reviews,
    latitude,
    longitude,
    features
  )

  
  
  
  
data_full_cleaned_tbl %>% glimpse()
```









# 3.0 SAVE & DISCONNECT

## Save to db
```{r}
ds_conn <- 
  DBI::dbConnect(
    RMariaDB::MariaDB(),
    host     = "rented-datascience.ccem57tyvghb.us-west-2.rds.amazonaws.com",
    dbname   = "rentaldata_dev",
    username = "david",
    password = "NZg*2ptvQ@X$uR")



## handy little function
dbWriteTableMySQLFast<- function(conn, df, tbl_name) {
  dbRemoveTable(conn, tbl_name, fail_if_missing = FALSE)
  dbCreateTable(conn, tbl_name, fields = df)
  
  f <- tempfile()
  write_csv(df, file = f, na = "NULL")
  
  dbWriteTable(conn, tbl_name, f, append = TRUE)

  unlink(f)
}


# upload
dbWriteTableMySQLFast(
  conn     = ds_conn, 
  df       = data_full_cleaned_tbl, 
  tbl_name = "00_art_listing_ids_cleaned")



# disconnect
dbDisconnect(ds_conn)
```


## Save as parquet
```{r}
# live file in scheduled/00_data folder
write_parquet(data_full_cleaned_tbl, "C:\\Users\\DavidStephens\\Desktop\\Github\\scheduled\\00_data\\art_listing_ids_cleaned.parquet")


# live file for shiny in predicthq event dashboard
write_parquet(data_full_cleaned_tbl, "C:\\Users\\DavidStephens\\Desktop\\Github\\predicthq-streamlit\\00_data\\art_listing_ids_cleaned.parquet")


# live file for shiny in art_replica/shiny folder
write_parquet(data_full_cleaned_tbl, "C:\\Users\\DavidStephens\\Desktop\\Github\\artr_replica\\shiny\\art_listing_ids_cleaned.parquet")


# live VTRIPS ONLY file for shiny in art_replica/shiny folder
write_rds(data_full_cleaned_tbl %>%
                filter(account_id %in% c('1365', '1386', '1390', '1391', '1392')),
              "C:\\Users\\DavidStephens\\Desktop\\Github\\artr_replica\\shiny\\vtrips_art_listing_ids_cleaned.rds")




# time-stamped archive file
write_parquet(data_full_cleaned_tbl, str_glue("C:\\Users\\DavidStephens\\Desktop\\Github\\scheduled\\00_data\\archive\\{date}_art_listing_ids_cleaned.parquet"))
```


## Disconnect SQL
```{r}
dbDisconnect(prod_conn)
```




# 4.0 EMAIL NOTIFICATION
```{r}

date_time <- add_readable_time()

body_text <- 
  md(str_glue(
    
"
Good news!

The **rentedrmsprod: Synced Listings Only** script ran successfully on {date_time}.

"))


footer_text <- md("sent via the [blastula](https://rstudio.github.io/blastula) R package")



email <- compose_email(
  body = body_text,
  footer = footer_text
)


# send the email
email %>%
  smtp_send(
    from = "dstephens@tnsinc.com",
    to = "dstephens@tnsinc.com", 
    subject = str_glue("Synced Listings Only"),
    credentials = creds_file(file = "C:\\Users\\DavidStephens\\Desktop\\Github\\scheduled\\gmail_creds")
  )
```










