---
title: "rentedrmsprod Nested Forecast"
---


# LIBRARIES
```{r setup, include=FALSE}

# sql connect
library(odbc)
library(DBI)
library(arrow)

# core packages
library(tidyverse)
library(dbplyr)
library(timetk)
library(tidyquant)
library(janitor)
library(lubridate)
library(stringi)
library(jsonlite)
library(tidyjson)
library(data.table)
library(arrow)

# visualization
library(gt)
library(scales)
library(plotly)

# time series ml
library(tidymodels)
library(modeltime)
library(modeltime.ensemble)
library(modeltime.resample)
library(anomalize)
library(prophet)
library(rules)
library(trelliscopejs)
library(ranger)
library(randomForest)
library(recipes)
library(kknn)
library(kernlab)
library(thief)
library(Cubist)

# Timing & Parallel Processing
library(future)
library(doFuture)
library(parallel)
library(bundle)

library(blastula)


source("C:\\Users\\DavidStephens\\Desktop\\Github\\artr_replica\\shiny\\R\\ds1_new_model_functions.R")
source("C:\\Users\\DavidStephens\\Desktop\\Github\\artr_replica\\shiny\\R\\ds1_calendar_functions.R")


date <- today()
options(scipen = 9999)


start_date <- as.Date('2020-01-01')
end_date <- as.Date(today() + 120)

end_date_forecast <- as.Date(today() + 365)
horizon <- interval(end_date, end_date_forecast) %/% days(1)



DS_HOST    <- Sys.getenv("DS_HOST")
DS_DB_NAME <- Sys.getenv("DS_DB_NAME")
SQL_ID     <- Sys.getenv("SQL_ID")
SQL_PW     <- Sys.getenv("SQL_PW")

conn <- dbConnect(RMariaDB::MariaDB(),
                  host     = DS_HOST,
                  dbname   = DS_DB_NAME,
                  username = SQL_ID,
                  password = SQL_PW)



PROD_HOST    <- Sys.getenv("PROD_HOST")
PROD_DB_NAME <- Sys.getenv("PROD_DB_NAME")
PROD_ID      <- Sys.getenv("PROD_ID")
PROD_PW      <- Sys.getenv("PROD_PW")

prod_conn <- dbConnect(RMariaDB::MariaDB(),
                        host     = PROD_HOST,
                        dbname   = PROD_DB_NAME,
                        username = PROD_ID,
                        password = PROD_PW)

knitr::opts_chunk$set(echo = TRUE)
```




# --
# 0.0 SQL DATA
## pull art_listings: geomarket & rates
```{sql connection=prod_conn, output.var="ds_historical_listing_rates"}
select *
from ds_historical_listing_rates
```


## pull art_groups
```{sql connection=prod_conn, output.var="art_listings_bookings"}
select listings_id, type, created_date, text, rate_type, rate, booking_date
from art_listings_bookings
```


## Raw Active Listings Data
```{r}
art_listings_raw <- read_parquet("C:\\Users\\DavidStephens\\Desktop\\Github\\artr_replica\\shiny\\art_listing_ids_cleaned.parquet") 

art_listings_raw %>% glimpse()
```




## External Regressor Data
```{r}
holidays_prepared <- prepare_holidays(conn, country = "US")

inflation_join <- read_rds("C:\\Users\\DavidStephens\\Desktop\\Github\\artr_replica\\shiny\\inflation.rds") %>%
  dplyr::mutate(inflation = log1p(inflation)) 

```



## Disconnect SQL
```{r}
dbDisconnect(prod_conn)
dbDisconnect(conn)
```




# --
# 1.0 DATA PREP
## art_listings_raw
```{r}

# cleaned active listings
art_listings_cleaned <- art_listings_raw %>%
  select(id, listing_name, beds, full_baths, 
         guests, rating, reviews, features, gap_stay, 
         latitude, longitude, min_stay, bed_count, 
         room_type, gap_weekends) %>%
  rename(listing_id = id)

```


## art_listings_bookings
```{r}

bookings_prepared_tbl <- art_listings_bookings %>%
  rename(listing_id = listings_id) %>%
  filter(type == "booked" | type == "booking") %>%
  
  # rate cut-off is set at under $45/night, pdw Mike
  mutate(rate = round(rate),
         rate = ifelse(rate < 45, NA, rate),
         days_out = booking_date - created_date,
         rate_version = "bookings") %>%
  rename(date = booking_date) %>%
  select(listing_id, date, rate, rate_version) %>%
  drop_na()
  
```




## Visualize
```{r, eval=FALSE}

# ds_historical_listing_rates %>%
#   group_by(listing_id) %>%
#   pivot_longer(cols = contains("rate")) %>%
#   plot_time_series(
#     date, value, 
#     .color_var = name, 
#     .smooth = F, 
#     .facet_ncol = 3,
#     .facet_nrow = 1,
#     .trelliscope = T
#   )

```


## Create missing prior year
```{r}
ds_rates_prepared <- ds_historical_listing_rates %>%
  
  # keep display_rate if within 4 months, otherwise predicted_rate
  mutate(rate = ifelse(date <= today() + 120, display_rate, predicted_rate)) %>%
  select(date, listing_id, rate) %>%
  
  # impute NA's & zero values
  group_by(listing_id) %>%
  mutate(rate = ifelse(rate <= 0, NA, rate),
         rate = ts_impute_vec(rate, period = 7)) %>%
  ungroup() %>%
  
  # pad time for days prior
  group_by(listing_id) %>%
  pad_by_time(
    date, 
    .by         = "day",
    .start_date = start_date,
    .end_date   = max(ds_historical_listing_rates$date),
    .pad_value  = NA
  ) %>%
  
  ## create prior year data with +8% increase YoY
  # 1-year delay
  tk_augment_leads(rate, .lags = -365, .names = "rate_lead_1") %>%
  mutate(rate_lead_1 = round(rate_lead_1 * 0.92)) %>%
  
  # 2-year delay
  tk_augment_leads(rate_lead_1, .lags = -365, .names = "rate_lead_2") %>%
  mutate(rate_lead_2 = round(rate_lead_2 * 0.92)) %>%
  
  # 3-year delay
  tk_augment_leads(rate_lead_2, .lags = -365, .names = "rate_lead_3") %>%
  mutate(rate_lead_3 = round(rate_lead_3 * 0.92)) %>%
  
  # 4-year delay
  tk_augment_leads(rate_lead_3, .lags = -365, .names = "rate_lead_4") %>%
  mutate(rate_lead_4 = round(rate_lead_4 * 0.92)) %>%

  # join all rates into 1 column
  mutate(rate = case_when(
    !is.na(rate) ~ rate,
    is.na(rate) & !is.na(rate_lead_1) ~ rate_lead_1,
    is.na(rate) & is.na(rate_lead_1) & !is.na(rate_lead_2) ~ rate_lead_2,
    is.na(rate) & is.na(rate_lead_1) & is.na(rate_lead_2) & !is.na(rate_lead_3) ~ rate_lead_3,
    is.na(rate) & is.na(rate_lead_1) & is.na(rate_lead_2) & is.na(rate_lead_3) & !is.na(rate_lead_4) ~ rate_lead_4,
    T ~ rate
  )) %>%
  
  # impute any missing data
  mutate(rate = round(ts_impute_vec(rate, period = 7))) %>%
  ungroup() %>%
  select(-rate_lead_1, -rate_lead_2, -rate_lead_3, -rate_lead_4) %>%
  mutate(rate_version = "historical")



# create filter for only the listing_ids in ds_historical_listing_rates dataset
ds_rates_prepared_vector <- ds_rates_prepared %>% 
  distinct(listing_id) %>% 
  pull(listing_id) %>% 
  as.vector()
```



## Bind with bookings
```{r}
# remove any "historical" dates with "bookings"
ds_rates_joined <- ds_rates_prepared %>%
  bind_rows(bookings_prepared_tbl %>% filter(listing_id %in% ds_rates_prepared_vector)) %>%
  group_by(listing_id, date) %>%
  arrange(listing_id, date, rate_version) %>%
  distinct(date, .keep_all = T) %>%
  ungroup()





# visualize
# ds_rates_joined %>%
#   group_by(listing_id) %>%
#   plot_time_series(
#     date, rate,
#     .facet_ncol = 3,
#     .facet_nrow = 2,
#     .trelliscope = T,
#     .smooth = F
#   )


# save for global forecast
write_rds(ds_rates_joined, str_glue("00_data/{date}_ds_rates_joined.rds"))
```



## Create Full Dataset
```{r}
full_data_tbl <- ds_rates_joined %>%
  
  # global changes
  mutate(rate = log1p(rate)) %>%
  select(-rate_version) %>%
  arrange(listing_id, date) %>%
  
  # filter out data with less than 1 year of data & by time.
  group_by(listing_id) %>%
  filter(n() >= 365) %>%
  filter_by_time(
    date,
    .start_date = start_date,
    .end_date   = end_date
  ) %>%
  filter(n() >= 365) %>%
  ungroup() %>%
  
  # create future frame
  group_by(listing_id) %>%
  future_frame(
    .date_var   = date,
    .length_out = horizon,
    .bind_data  = TRUE) %>%
  ungroup() %>%
  
  # fourier, lags & slidify
  group_by(listing_id) %>%
  group_split() %>%
  map(.f = function(df) {
    df %>%
      arrange(date) %>%
      tk_augment_fourier(date, .periods = c(7, 30, 90, 365)) %>%
      tk_augment_lags(rate, .lags = horizon) %>%
      tk_augment_slidify(
        str_glue("rate_lag{horizon}"),
        .f       = ~mean(.x, na.rm = TRUE),
        .period  = c(7, 30, 90),
        .partial = TRUE,
        .align   = "center"
      )
  }) %>%
  bind_rows() %>%

  # add xregs
  left_join(holidays_prepared, by = c("date" = "ds")) %>%
  left_join(art_listings_cleaned, by = "listing_id") %>%
  left_join(inflation_join, by = "date") %>%
  
  # add anomalies
  group_by(listing_id) %>%
  ungroup() %>%
  
  # drop na's from lag
  drop_na(str_glue("rate_lag{horizon}"))




# visualize
# full_data_tbl %>%
#   group_by(listing_id) %>%
#   plot_time_series(
#     date, rate,
#     .facet_ncol = 3,
#     .facet_nrow = 2,
#     .smooth = F,
#     .trelliscope = T)

```




# --
# 2.0 NESTED TIME SERIES
```{r}
nested_data_tbl <- full_data_tbl %>%
  
  nest_timeseries(
    .id_var = listing_id,
    .length_future = horizon
  ) %>%
  
  split_nested_timeseries(
    .length_test = horizon
  )
```




# --
# 3.0 RECIPES
```{r}

# sequential model recipe - date included
recipe_spec <- recipe(rate ~ ., extract_nested_train_split(nested_data_tbl)) %>%
  step_timeseries_signature(date) %>%
  step_rm(listing_name, features) %>%
  step_string2factor(room_type) %>%
  step_zv(all_predictors()) %>%
  step_rm(matches("(.xts$)|(.iso$)|(hour)|(minute)|(second)|(am.pm)")) %>%
  step_normalize(matches("(index.num)|(year)|(yday)")) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE)

recipe_spec %>%
  prep() %>%
  juice() %>%
  glimpse()


# machine learning recipe - no date
recipe_spec_ml <- recipe(rate ~ ., extract_nested_train_split(nested_data_tbl)) %>%
  step_timeseries_signature(date) %>%
  step_rm(listing_name, features) %>%
  step_rm(date) %>%
  step_string2factor(room_type) %>% 
  step_zv(all_predictors()) %>%
  step_rm(matches("(.xts$)|(.iso$)|(hour)|(minute)|(second)|(am.pm)")) %>%
  step_normalize(matches("(index.num)|(year)|(yday)")) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE)
  
recipe_spec_ml %>%
  prep() %>%
  juice() %>%
  glimpse()


bake(prep(recipe_spec), extract_nested_train_split(nested_data_tbl))
bake(prep(recipe_spec_ml), extract_nested_train_split(nested_data_tbl))
```




# --
# 4.0 ML MODELS
## Parallel Processing
```{r}
parallel_stop()

detectCores()
doParallel::registerDoParallel(cores = 15)
```


## ML Models
```{r}
# XGBoost Models
set.seed(5678)
wflw_xgb_1 <- workflow() %>%
  add_model(boost_tree(
    mode = "regression",
    mtry = 43,
    trees = 1000,
    min_n = 28,
    tree_depth = 2,
    learn_rate = 0.3,
    loss_reduction = 0
  ) %>%
    set_engine("xgboost")) %>%
  add_recipe(recipe_spec_ml)



# ARIMA Boost
set.seed(5678)
wflw_arima_boost <- workflow() %>%
  add_model(arima_boost(
    seasonal_period = 7,
    
    # xgboost
    mtry = 43,
    trees = 1000,
    min_n = 28,
    tree_depth = 2,
    learn_rate = 0.3,
    loss_reduction = 0
    )  %>%
  set_engine("arima_xgboost")) %>%
  add_recipe(recipe_spec)
```





# --
# 4.1 MODELS W/TUNE
## Parallel Processing
```{r}
doParallel::stopImplicitCluster()
doParallel::registerDoParallel(cores = 15)
```



## Resamples - K-Fold
```{r}
set.seed(5678)
resamples_kfold <- extract_nested_train_split(nested_data_tbl) %>% vfold_cv(v = 5)

resamples_kfold %>%
  tk_time_series_cv_plan() %>%
  plot_time_series_cv_plan(
    .date_var = date, 
    .value = rate, 
    .facet_ncol = 2
  )
```


## XGBoost
```{r}
model_spec_xgboost_tune <- boost_tree(
  mode           = "regression",
  trees          = 1000,
  mtry           = tune(),
  min_n          = tune(),
  tree_depth     = tune(),
  learn_rate     = tune(),
  loss_reduction = tune()
) %>%
  set_engine("xgboost")
  
wflw_spec_xgboost_tune <- workflow() %>%
  add_model(model_spec_xgboost_tune) %>%
  add_recipe(recipe_spec_ml)


# ** Tuning
set.seed(5678)
tune_results_xgboost <- wflw_spec_xgboost_tune %>%
  tune_grid(
    resamples = resamples_kfold,
    param_info = extract_parameter_set_dials(wflw_spec_xgboost_tune) %>%
      update(
        learn_rate = learn_rate(range = c(0.001, 0.400), trans = NULL)
      ),
    grid = 10,
    control = control_grid(verbose = TRUE, allow_par = TRUE)
  )


# ** Results
tune_results_xgboost %>% show_best("rmse", n = Inf)


# ** Finalize
wflw_xgboost_tuned <- wflw_spec_xgboost_tune %>%
  finalize_workflow(select_best(tune_results_xgboost, "rmse"))
```





# --
# 5.0 TESTING
## Start with 1 time series
```{r, eval=FALSE}
doParallel::stopImplicitCluster()
doParallel::registerDoParallel(cores = 15)

sample_tbl <- nested_data_tbl %>%
  slice(1:5) %>%
  modeltime_nested_fit(
    
    model_list = list(
      wflw_xgb_1,
      wflw_arima_boost,
      wflw_xgboost_tuned
    ),
    
    control = control_nested_fit(
      verbose = TRUE,
      allow_par = TRUE
    )
  )

# check for errors
sample_tbl %>% extract_nested_error_report()
```



## Scale to all time series
```{r}
doParallel::stopImplicitCluster()
doParallel::registerDoParallel(cores = 15)


nested_modeltime_tbl <- nested_data_tbl %>%
  modeltime_nested_fit(
    
    model_list = list(
      wflw_xgb_1,
      wflw_arima_boost,
      wflw_xgboost_tuned
    ),
    
    control = control_nested_fit(
      verbose = TRUE,
      allow_par = TRUE
    )
  )

# check for errors
nested_modeltime_tbl %>% extract_nested_error_report()
```


## Check Accuracy & Errors
```{r}
# check for errors
error_report <- nested_modeltime_tbl %>% extract_nested_error_report()
ids_small_timeseries <- as.vector(unique(error_report$listing_id))


# review non-errors nest
nested_modeltime_tbl %>%
  filter(!listing_id %in% ids_small_timeseries) %>%
  extract_nested_train_split()


# check accuracy on testing data
nested_modeltime_tbl %>%
  extract_nested_test_accuracy() %>%
  table_modeltime_accuracy()


# visualize
# nested_modeltime_tbl %>%
#   extract_nested_test_forecast() %>%
#   group_by(listing_id) %>%
#   plot_modeltime_forecast(
#     .facet_ncol = 3,
#     .trelliscope = TRUE,
#     .conf_interval_show = FALSE
#   )


# separate from errors for clean nest
nested_modeltime_subset_tbl <- nested_modeltime_tbl %>%
  filter(!listing_id %in% ids_small_timeseries)
```



# --
# 6.0 SELECT BEST
```{r}
#### BEST SELECTED ####
nested_best_tbl <- nested_modeltime_subset_tbl %>% 
  modeltime_nested_select_best(
    metric = "rmse",
    minimize = TRUE,
    filter_test_forecasts = TRUE)


# best report
nested_best_tbl %>% extract_nested_best_model_report()


# visualize
# nested_best_tbl %>%
#   extract_nested_test_forecast() %>%
#   group_by(listing_id) %>%
#   plot_modeltime_forecast(
#     .facet_ncol = 3,
#     .conf_interval_show = FALSE,
#     .trelliscope = T)
```




# --
# 7.0 REFIT & FORECAST
```{r}
doParallel::stopImplicitCluster()
doParallel::registerDoParallel(cores = 15)



### ONLY THE BEST MODELS ###
nested_best_refit_tbl <- nested_best_tbl %>%
  modeltime_nested_refit(
    control = control_nested_refit(
      verbose = TRUE,
      allow_par = TRUE
    )
  )



# check for errors (should be zero)
nested_best_refit_tbl %>% extract_nested_error_report()
```


## Unnest & Prepare for Export
```{r}
pct_increase <- 10


# prepare for export
unnested_best_refit_tbl <- nested_best_refit_tbl %>%
  extract_nested_future_forecast() %>%
  mutate(across(
    .cols = c(.value:.conf_hi),
    .fns  = expm1
  )) %>%
  rename(date  = .index, 
         value = .value,
         type  = .key) %>%
  select(listing_id, date, value, type)





# increasing the yoy forecast for each week
forecast_increase <- unnested_best_refit_tbl %>%
  
  # add year & week, and summarize
  tk_augment_timeseries_signature(date) %>%
  group_by(listing_id, year, week) %>%
  summarize(y_week = mean(value, na.rm = T)) %>%
  ungroup() %>%
  
  # increase all xgboost forecasts by week (if not already done so)
  group_by(listing_id, week) %>%
  mutate(pct_chg    = (y_week / lag(y_week, 1)) - 1,
         yoy_flat   = ifelse(pct_chg >= (pct_increase/100), y_week, (y_week/(1-abs(pct_chg))) * (1 + as.numeric(pct_increase/100))),
         multiplier = (yoy_flat / y_week)) %>%
  ungroup()



# left_join to show prediction columns
pct_increase_identifier <- unnested_best_refit_tbl %>%
  tk_augment_timeseries_signature(date) %>%
  group_by(listing_id, year, week, type) %>%
  distinct(listing_id, year, week, type) %>%
  ungroup() %>%
  arrange(listing_id, year, week) %>%
  filter(type == 'prediction') %>%
  left_join(forecast_increase, by = c('listing_id', 'year', 'week')) %>%
  select(listing_id, year, week, multiplier)




# join all & include increase
unnested_prepared_best_refit_tbl <- unnested_best_refit_tbl %>%
  tk_augment_timeseries_signature(date) %>%
  select(listing_id, date, value, type, year, week) %>%
  left_join(pct_increase_identifier, by = c('listing_id', 'year', 'week')) %>%
  arrange(listing_id, year, week) %>%
  mutate(value = ifelse(is.na(multiplier), value, value * multiplier)) %>%
  select(listing_id, date, value, type)
  
  


# visualize future
# unnested_prepared_best_refit_tbl %>%
#   group_by(listing_id) %>%
#   plot_time_series(
#     date, value,
#     .color_var   = type,
#     .smooth      = F,
#     .facet_ncol  = 3,
#     .facet_nrow  = 2,
#     .trelliscope = T)


doParallel::stopImplicitCluster()
```




# --
# 7.1 SAVE & EXPORT
```{r}
# live file
unnested_prepared_best_refit_tbl %>% write_rds("C:\\Users\\DavidStephens\\Desktop\\Github\\artr_replica\\shiny\\ds_historical_listing_rates_forecast.rds")


# timestamp archive
unnested_prepared_best_refit_tbl %>% write_rds(str_glue("C:\\Users\\DavidStephens\\Desktop\\Github\\artr_replica\\00_data\\{date}_ds_historical_listing_rates_forecast.rds"))
```






# --
# 8.0 EMAIL NOTIFICATION
```{r}

date_time <- add_readable_time()

body_text <- 
  md(str_glue(
    
"
Good news!

The **rentedrmsprod Nested Forecast** script ran successfully on {date_time}.

"))


footer_text <- md("sent via the [blastula](https://rstudio.github.io/blastula) R package")



email <- compose_email(
  body = body_text,
  footer = footer_text
)


# send the email
email %>%
  smtp_send(
    from = "dstephens@tnsinc.com",
    to = "dstephens@tnsinc.com", 
    subject = str_glue("Synced Only Forecast"),
    credentials = creds_file(file = "C:\\Users\\DavidStephens\\Desktop\\Github\\scheduled\\gmail_creds")
  )
```







